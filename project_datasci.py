# -*- coding: utf-8 -*-
"""project_Datasci.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XDeTuF4xOOatGGh_25qVCJqHl6Ex8M23

# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1 : ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏†‡∏π‡∏°‡∏¥‡∏ó‡∏±‡∏®‡∏ô‡πå‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö

‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1 : ‡∏Ñ‡∏±‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡πÄ‡∏•‡∏∞‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
-‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 8 ‡πÄ‡πÄ‡∏´‡πà‡∏á ‡πÄ‡∏ä‡πà‡∏ô ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏™‡∏á‡∏Ç‡∏•‡∏≤‡∏ô‡∏Ñ‡∏£‡∏¥‡∏ô‡∏ó‡∏£‡πå , ‡∏à‡∏∏‡∏¨‡∏≤‡∏•‡∏á‡∏Å‡∏£‡∏ì‡πå‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢ , ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏°‡∏´‡∏¥‡∏î‡∏• , ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà , ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏û‡∏£‡∏∞‡∏à‡∏≠‡∏°‡πÄ‡∏Å‡∏•‡πâ‡∏≤‡∏ò‡∏ô‡∏ö‡∏∏‡∏£‡∏µ , ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏Ç‡∏≠‡∏ô‡πÅ‡∏Å‡πà‡∏ô , ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏û‡∏£‡∏∞‡∏à‡∏≠‡∏°‡πÄ‡∏Å‡∏•‡πâ‡∏≤‡∏û‡∏£‡∏∞‡∏ô‡∏Ñ‡∏£‡πÄ‡∏´‡∏ô‡∏∑‡∏≠ , ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÅ‡∏°‡πà‡∏ü‡πâ‡∏≤‡∏´‡∏•‡∏ß‡∏á
-‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡πÄ‡∏•‡∏∞‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
"""

import json
import csv
from datetime import datetime

# ===== ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ =====
input_file = 'arxiv-metadata-oai-snapshot.json'
output_file = 'filtered_arxiv_with_universities2.csv'
target_categories = ['cs.AI', 'cs.LG', 'cs.CV']  # ‡∏´‡∏°‡∏ß‡∏î‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
start_year, end_year = 2014, 2024

# ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
target_universities = {
    'PSU': ['Prince of Songkla', 'PSU Thailand', 'Songkla Univ','Prince of Songkla University','PSU'],
    'CU': ['Chulalongkorn University', 'Chula University', 'Chula','CU'],
    'MU': ['Mahidol University', 'Mahidol U', 'Mahidol','MU'],
    'CMU': ['Chiang Mai University', 'CMU','Chiang Mai'],
    'KMUTT': ['King Mongkut\'s University of Technology Thonburi', 'KMUTT'],
    'KKU': ['KKU','Khon Kaen University','Khon Kaen'],
    'KMUTNB': ['KMUTNB','King Mongkut\'s University of Technology North Bangkok'],
    'MFU': ['MFU','Mae Fah Luang University','Mae Fah Luang']
}

def find_affiliated_university(authors_text):
    if not authors_text:
        return None
    text = str(authors_text).lower()
    for uni_code, uni_names in target_universities.items():
        for name in uni_names:
            if name.lower() in text:
                return uni_code
    return None

# ===== ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô Header CSV =====
with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(['id', 'title', 'categories', 'update_date', 'abstract', 'authors', 'university_code'])

# ===== ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö Streaming =====
with open(input_file, 'r', encoding='utf-8') as f:
    count = saved = 0

    for line in f:
        count += 1
        try:
            record = json.loads(line)
        except json.JSONDecodeError:
            continue

        # --- 1Ô∏è‚É£ ‡∏Å‡∏£‡∏≠‡∏á‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà ---
        categories = record.get('categories', '')
        if not any(cat in categories for cat in target_categories):
            continue

        # --- 2Ô∏è‚É£ ‡∏Å‡∏£‡∏≠‡∏á‡∏õ‡∏µ ---
        date_str = record.get('update_date', '')
        try:
            year = datetime.strptime(date_str, '%Y-%m-%d').year
        except:
            continue
        if year < start_year or year > end_year:
            continue

        # --- 3Ô∏è‚É£ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÉ‡∏ô authors ---
        authors_text = record.get('authors', '') or record.get('authors_parsed', '')
        uni_code = find_affiliated_university(authors_text)
        if not uni_code:
            continue  # ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ ‚Üí ‡∏Ç‡πâ‡∏≤‡∏°

        # --- 4Ô∏è‚É£ ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏•‡∏á CSV ---
        authors_clean = str(record.get('authors', '')).replace('\n', ' ').strip()

        with open(output_file, 'a', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow([
                record.get('id', ''),
                record.get('title', '').replace('\n', ' '),
                categories,
                date_str,
                record.get('abstract', '').replace('\n', ' '),
                authors_clean,
                uni_code
            ])
        saved += 1

        if count % 100000 == 0:
            print(f'‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß {count:,} ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î | ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ {saved:,} ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°')

print(f'‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {saved:,} ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢')
print(f'üìÑ ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ: {output_file}')

# =============================
# üìä EDA & Visualization for ArXiv Filtered Dataset
# =============================
import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx
from itertools import combinations
from collections import Counter

# üìå 1) Load dataset
csv_path = 'filtered_arxiv_with_universities2.csv'   # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô path ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
df = pd.read_csv(csv_path, dtype=str, low_memory=False)

# --- Cleaning & preparation ---
df['update_date'] = pd.to_datetime(df['update_date'], errors='coerce')
df['year'] = df['update_date'].dt.year
df['categories'] = df['categories'].fillna('')
df['authors'] = df['authors'].fillna('')

print("‚úÖ Dataset loaded")
print("Records:", len(df))
print("Universities distribution:")
print(df['university_code'].value_counts())

"""‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2 : ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏≥‡∏£‡∏ß‡∏à‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö"""

-‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏ú‡∏•‡∏á‡∏≤‡∏ô: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏á‡∏≤‡∏ô‡∏ï‡∏µ‡∏û‡∏¥‡∏°‡∏û‡πå‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏õ‡∏µ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢

# üìà 2) Publications per year per university
# =============================
pubs_per_year = df.groupby(['university_code', 'year']).size().unstack(fill_value=0)

plt.figure(figsize=(12, 6))
for uni in pubs_per_year.index:
    plt.plot(pubs_per_year.columns, pubs_per_year.loc[uni], marker='o', label=uni)
plt.xlabel('Year')
plt.ylabel('Number of publications')
plt.title('Publications per Year per University')
plt.legend()
plt.tight_layout()
plt.show()

"""-‡∏™‡∏≤‡∏Ç‡∏≤‡∏ó‡∏µ‡πà‡πÇ‡∏î‡∏î‡πÄ‡∏î‡πà‡∏ô: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏û‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢ (Categories) ‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç"""

# üß† 3) Top Categories per University
# =============================
# ‡πÅ‡∏¢‡∏Å categories ‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô list
def split_categories(cat_str):
    if not isinstance(cat_str, str):
        return []
    return [c.strip() for c in cat_str.split() if c.strip()]

df['cat_list'] = df['categories'].apply(split_categories)
df_exploded = df.explode('cat_list')

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Top 10 categories ‡∏ó‡∏±‡πâ‡∏á dataset
top_categories = (
    df_exploded['cat_list'].value_counts()
    .head(10)
    .index
    .tolist()
)

# ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ category ‡∏ï‡πà‡∏≠‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢
cats_by_uni = {}
for uni in df['university_code'].unique():
    subset = df_exploded[df_exploded['university_code'] == uni]
    cnt = Counter(subset['cat_list'])
    cats_by_uni[uni] = [cnt.get(c, 0) for c in top_categories]

# ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏ö‡∏ö grouped bar chart
plt.figure(figsize=(14, 6))
x = range(len(top_categories))
width = 0.1

for i, uni in enumerate(sorted(cats_by_uni.keys())):
    offsets = [xi + (i - len(cats_by_uni)/2)*width for xi in x]
    plt.bar(offsets, cats_by_uni[uni], width=width, label=uni)

plt.xticks(x, top_categories, rotation=45, ha='right')
plt.xlabel('Categories')
plt.ylabel('Number of publications')
plt.title('Top 10 Categories by University')
plt.legend()
plt.tight_layout()
plt.show()

"""‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πà‡∏ß‡∏°‡∏°‡∏∑‡∏≠: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏ú‡∏π‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏£‡πà‡∏ß‡∏° (Co-authorship) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πà‡∏ß‡∏°‡∏°‡∏∑‡∏≠‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô"""

# üë• 4) Co-authorship Network (Author-level)
# =============================
from networkx.algorithms import community
import matplotlib.cm as cm

def split_authors(s):
    """Split authors string into list"""
    if not isinstance(s, str) or s.strip() == '':
        return []
    # split by common separators
    if ';' in s:
        return [a.strip() for a in s.split(';') if a.strip()]
    if ' and ' in s:
        return [a.strip() for a in s.split(' and ') if a.strip()]
    if ',' in s:
        return [a.strip() for a in s.split(',') if a.strip()]
    return [s.strip()]

G = nx.Graph()

for _, row in df.iterrows():
    authors = split_authors(row['authors'])
    authors = [a for a in authors if a != '']
    for a in authors:
        if not G.has_node(a):
            G.add_node(a)
    for a, b in combinations(authors, 2):
        if G.has_edge(a, b):
            G[a][b]['weight'] += 1
        else:
            G.add_edge(a, b, weight=1)

print(f"Total authors in graph: {len(G)}")

# ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏â‡∏û‡∏≤‡∏∞ top 30 nodes ‡∏ó‡∏µ‡πà‡∏°‡∏µ degree ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
deg = dict(G.degree())
top_authors = sorted(deg, key=deg.get, reverse=True)[:30]
subG = G.subgraph(top_authors)

subG = G.subgraph(top_authors).copy()
communities = list(community.greedy_modularity_communities(subG))

# ‡∏™‡∏£‡πâ‡∏≤‡∏á mapping: node -> community index
community_map = {}
for i, comm in enumerate(communities):
    for node in comm:
        community_map[node] = i

colors = [community_map[node] for node in subG.nodes()]

plt.figure(figsize=(12, 10))
pos = nx.spring_layout(subG, seed=42)
nx.draw_networkx_nodes(subG, pos, node_color=colors, cmap=cm.tab20, node_size=150)
nx.draw_networkx_edges(subG, pos, alpha=0.4)
nx.draw_networkx_labels(subG, pos, font_size=8)

plt.title('Co-authorship Network (Top 30 authors)\nColored by collaboration communities')
plt.axis('off')
plt.tight_layout()
plt.show()

from networkx.algorithms import community
import matplotlib.cm as cm
import matplotlib.pyplot as plt
import networkx as nx

# --- 1) ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Top 20 authors ---
deg = dict(G.degree())
top_authors = sorted(deg, key=deg.get, reverse=True)[:20]
subG = G.subgraph(top_authors).copy()

# --- 2) ‡∏´‡∏≤ communities ---
communities = list(community.greedy_modularity_communities(subG))
community_map = {}
for i, comm in enumerate(communities):
    for node in comm:
        community_map[node] = i
colors = [community_map[node] for node in subG.nodes()]

# --- 3) Layout: ‡∏õ‡∏£‡∏±‡∏ö k ‡πÉ‡∏´‡πâ node ‡∏´‡πà‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô ---
# ‡∏Ñ‡πà‡∏≤ k ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á nx.spring_layout ‡∏Ñ‡∏∑‡∏≠ 1/sqrt(n)
# ‡∏•‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô 0.5 - 1.5 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ node ‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
pos = nx.spring_layout(subG, seed=42, k=0.7, iterations=100)

# --- 4) ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≤‡∏ü ---
plt.figure(figsize=(14, 10))
nx.draw_networkx_nodes(subG, pos, node_color=colors, cmap=cm.tab20, node_size=250, alpha=0.9)
nx.draw_networkx_edges(subG, pos, alpha=0.4)
nx.draw_networkx_labels(subG, pos, font_size=9)

plt.title('Co-authorship Network (Top 20 authors)\n Colored by collaboration communities')
plt.axis('off')
plt.tight_layout()
plt.show()

"""# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2 ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ö‡∏ö‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏®‡∏±‡∏Å‡∏¢‡∏†‡∏≤‡∏û

‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏ô‡∏¥‡∏¢‡∏≤‡∏° "‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö" ‡πÄ‡πÄ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
-‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°
-‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
"""

import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer

df = pd.read_csv('filtered_arxiv_with_universities.csv')

# 1. ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô
df['num_authors'] = df['authors'].fillna('').apply(lambda x: len(x.split(',')))

# 2. ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ö‡∏ó‡∏Ñ‡∏±‡∏î‡∏¢‡πà‡∏≠
df['abstract_length'] = df['abstract'].fillna('').apply(lambda x: len(x.split()))

# 3. ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà (‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô list ‡πÅ‡∏•‡πâ‡∏ß one-hot)
df['categories_list'] = df['categories'].apply(lambda x: x.split())
mlb = MultiLabelBinarizer()
cat_df = pd.DataFrame(mlb.fit_transform(df['categories_list']), columns=mlb.classes_)
df = pd.concat([df, cat_df], axis=1)

# 4. ‡∏õ‡∏µ
df['year'] = pd.to_datetime(df['update_date']).dt.year

# 5. Target
# 1Ô∏è‚É£ ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏õ‡∏µ 2020‚Äì2024 ‚Üí ‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô 1
# ‚úÖ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏õ‡∏µ 2020‚Äì2024
df['impact_label'] = ((df['year'] >= 2020) & (df['year'] <= 2024)).astype(int)

# ‚úÖ ‡∏´‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ 'cs.'
cs_columns = ['cs.AI', 'cs.LG', 'cs.CV']

# ‚úÖ ‡∏ö‡∏ß‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏™‡∏≤‡∏Ç‡∏≤ cs. ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ 1
df['impact_label'] += df[cs_columns].sum(axis=1)

df

"""‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏™‡∏π‡∏á ‡∏Ñ‡∏∑‡∏≠‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ impact ‡∏™‡∏π‡∏á ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏∑‡∏≠‡∏´‡∏≤‡∏Å‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏õ‡∏µ 2020 ‡∏ñ‡∏∂‡∏á‡∏õ‡∏µ 2024 ‡πÉ‡∏´‡πâ1 ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏õ‡∏µ‡∏ô‡∏±‡πâ‡∏ô‡πÉ‡∏´‡πâ 0 ‡πÄ‡πÄ‡∏•‡∏∞‡∏ñ‡πâ‡∏≤‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏±‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏™‡∏≤‡∏Ç‡∏≤ cs.AI , cs.LG , cs.CV ‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏Ç‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞ 1 ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå impact_label"""

‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 4: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡πÄ‡∏ö‡∏ö‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (Baseline Model)
-Feature Engineering
-‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡πÄ‡∏ö‡∏ö‡∏à‡∏≥‡∏•‡∏≠‡∏á

"""Step 1 ‚Äî ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""

# ‚úÖ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î features ‡πÅ‡∏•‡∏∞ target
target = 'impact_label'

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å features ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
text_features = ['title', 'abstract']
numeric_features = ['year', 'num_authors', 'abstract_length']
categorical_features = ['university_code']

# One-hot categories ‡πÄ‡∏ä‡πà‡∏ô cs.AI, cs.LG, ...
category_cols = ['cs.AI', 'cs.LG', 'cs.CV']

X = df[text_features + numeric_features + categorical_features + category_cols]
y = df[target]

"""Step 2 ‚Äî ‡πÅ‡∏ö‡πà‡∏á‡∏ä‡∏∏‡∏î Train/Test"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

"""‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 5: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
-Feature Engineering

Step 3 ‚Äî ‡∏™‡∏£‡πâ‡∏≤‡∏á Baseline Model (Logistic Regression)
"""

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# ‚úÖ Preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('title_tfidf', TfidfVectorizer(max_features=3000, stop_words='english'), 'title'),
        ('abstract_tfidf', TfidfVectorizer(max_features=5000, stop_words='english'), 'abstract'),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),
        ('num', StandardScaler(), numeric_features + category_cols)
    ]
)

# ‚úÖ Logistic Regression Baseline
model = Pipeline(steps=[
    ('preprocess', preprocessor),
    ('clf', LogisticRegression(max_iter=500))
])

model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred))

"""Step 4 ‚Äî ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•"""

from sklearn.metrics import roc_auc_score

# AUC (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö classification)
if len(y.unique()) == 2:  # ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ binary
    y_prob = model.predict_proba(X_test)[:, 1]
    print("ROC AUC:", roc_auc_score(y_test, y_prob))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Logistic Regression')
plt.show()

"""Step 5 ‚Äî ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡πÄ‡∏ö‡∏ö‡∏à‡∏≥‡∏•‡∏≠‡∏á LightGBM"""

!pip install lightgbm

import lightgbm as lgb

lgbm_model = Pipeline(steps=[
    ('preprocess', preprocessor),
    ('clf', lgb.LGBMClassifier(
        n_estimators=300,
        learning_rate=0.05,
        max_depth=-1,
        random_state=42
    ))
])

lgbm_model.fit(X_train, y_train)
y_pred_lgbm = lgbm_model.predict(X_test)

print("LightGBM model performance:")
print(classification_report(y_test, y_pred_lgbm))